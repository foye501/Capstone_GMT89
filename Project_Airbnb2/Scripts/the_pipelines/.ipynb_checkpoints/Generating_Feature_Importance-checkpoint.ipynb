{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db28148",
   "metadata": {},
   "source": [
    "# 1) PCAed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa02598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import datetime\n",
    "import pickle\n",
    "from textwrap import dedent\n",
    "import sys\n",
    "sys.path = sys.path+['./utiles/']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, learning_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1751033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PCA_pre_processing_pipeline():\n",
    "    def __init__(self):\n",
    "        ## missing values\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        self.imputer = SimpleImputer(strategy='most_frequent')\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "#         X = X.rename(columns={i:i.replace(',','_').replace('>','_').replace('<','_') for i in X.columns})\n",
    "        from sklearn.decomposition import PCA\n",
    "        self.imputer.fit(X)\n",
    "        #### some homogeneous features can be reduced\n",
    "        ### 1) Amenities\n",
    "        ## specials\n",
    "        self.Amenities_specials = [\n",
    "         'Amenities_bedrooms',\n",
    "         'Amenities_beds',\n",
    "         'Amenities_minimum_nights',\n",
    "         'Amenities_maximum_nights',\n",
    "         'Amenities_property_type_code',\n",
    "         'Amenities_room_type_code',\n",
    "         'Amenities_bathrooms_count',\n",
    "         'Amenities_bathrooms_type_code',\n",
    "         'Amenities_amenities_count']\n",
    "        self.Amenities_rest = [i for i in X.columns if i.startswith('Amenities_') and i not in self.Amenities_specials]\n",
    "        self.Amenities_PCA = PCA(n_components=10, random_state=42)\n",
    "        print('Amenities_PCA...')\n",
    "        self.Amenities_PCA.fit(X[self.Amenities_rest])\n",
    "        \n",
    "        ### 2) Image\n",
    "        self.Image_specials = [\n",
    "        'Image_size',\n",
    "         'Image_sharpness',\n",
    "         'Image_mean_brightness',\n",
    "         'Image_contrast',\n",
    "        'Image_mean_mean_min_dist_r3',\n",
    "         'Image_mean_mean_weighted_min_dist_r3'\n",
    "        ]\n",
    "        self.Image_rest = [i for i in X.columns if i.startswith('Image_') and i not in self.Image_specials]\n",
    "        self.Image_PCA = PCA(n_components=10, random_state=42)\n",
    "        print('Image_PCA...')\n",
    "        self.Image_PCA.fit(X[self.Image_rest])\n",
    "        \n",
    "        ### 3) Location\n",
    "        self.Location_specials = [\n",
    "        'Location_supermarket_num',\n",
    "         'Location_restaurant_num',\n",
    "         'Location_cafe_500m_dis',\n",
    "         'Location_cafe_500m_num',\n",
    "         'Location_transport_most_close_dis',\n",
    "         'Location_transport_500m_num',\n",
    "         'Location_transport_1000m_num',\n",
    "        'Location_mean_area_accommodates_price',\n",
    "         'Location_mean_area_beds_price',\n",
    "         'Location_real_estate'\n",
    "        ]\n",
    "        self.Location_rest = [i for i in X.columns if i.startswith('Location_') and i not in self.Location_specials]\n",
    "        self.Location_PCA = PCA(n_components=5, random_state=42)\n",
    "        print('Location_PCA...')\n",
    "        self.Location_PCA.fit(X[self.Location_rest])\n",
    "        \n",
    "        ### 4) NLP\n",
    "        self.NLP_Embedding_list = [f'NLP_s{i}' for i in range(384)]\n",
    "        self.NLP_NER_list = [i for i in X.columns if i.startswith('NLP_') and i not in self.NLP_Embedding_list]\n",
    "        self.NLP_Embedding_PCA = PCA(n_components=20, random_state=42)\n",
    "        self.NLP_NER_PCA = PCA(n_components=10, random_state=42)\n",
    "        print('NLP_Embedding_PCA...')\n",
    "        self.NLP_Embedding_PCA.fit(X[self.NLP_Embedding_list])\n",
    "        print('NLP_NER_PCA...')\n",
    "        self.NLP_NER_PCA.fit(X[self.NLP_NER_list])\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "#         X = X.rename(columns={i:i.replace(',','_').replace('>','_').replace('<','_') for i in X.columns})\n",
    "        X[X.columns] = self.imputer.transform(X)\n",
    "        ### 1) transform Amenities\n",
    "        Amenities_pca_df = self.Amenities_PCA.transform(\n",
    "            X[self.Amenities_rest]\n",
    "        )\n",
    "        Amenities_pca_df = pd.DataFrame(\n",
    "            Amenities_pca_df, columns = [f'Amenities_PC{i+1}' for i in range(Amenities_pca_df.shape[1])]\n",
    "        )\n",
    "        \n",
    "        ### 2) transform Image\n",
    "        Image_pca_df = self.Image_PCA.transform(\n",
    "            X[self.Image_rest]\n",
    "        )\n",
    "        Image_pca_df = pd.DataFrame(\n",
    "            Image_pca_df, columns = [f'Image_PC{i+1}' for i in range(Image_pca_df.shape[1])]\n",
    "        )\n",
    "        \n",
    "        ### 3) transform Location\n",
    "        Location_pca_df = self.Location_PCA.transform(\n",
    "            X[self.Location_rest]\n",
    "        )\n",
    "        Location_pca_df = pd.DataFrame(\n",
    "            Location_pca_df, columns = [f'Location_PC{i+1}' for i in range(Location_pca_df.shape[1])]\n",
    "        )\n",
    "        \n",
    "        ### 4) transform NLP\n",
    "        NLP_pca_df1 = self.NLP_Embedding_PCA.transform(\n",
    "            X[self.NLP_Embedding_list]\n",
    "        )\n",
    "        NLP_pca_df1 = pd.DataFrame(\n",
    "            NLP_pca_df1, columns = [f'NLP_Embedding_PC{i+1}' for i in range(NLP_pca_df1.shape[1])]\n",
    "        )\n",
    "        \n",
    "        NLP_pca_df2 = self.NLP_NER_PCA.transform(\n",
    "            X[self.NLP_NER_list]\n",
    "        )\n",
    "        NLP_pca_df2 = pd.DataFrame(\n",
    "            NLP_pca_df2, columns = [f'NLP_NER_PC{i+1}' for i in range(NLP_pca_df2.shape[1])]\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            'X[self.Amenities_specials].shape:',X[self.Amenities_specials].shape,\n",
    "            'Amenities_pca_df.shape:',Amenities_pca_df.shape,\n",
    "            'X[self.Image_specials].shape:',X[self.Image_specials].shape,\n",
    "            'Image_pca_df.shape:',Image_pca_df.shape,\n",
    "            'X[self.Location_specials].shape:',X[self.Location_specials].shape,\n",
    "            'Location_pca_df.shape:',Location_pca_df.shape,\n",
    "            'NLP_pca_df1.shape:',NLP_pca_df1.shape,\n",
    "            'NLP_pca_df2.shape:',NLP_pca_df2.shape\n",
    "        )\n",
    "        \n",
    "        X = pd.concat([\n",
    "            X[self.Amenities_specials].reset_index(drop=True),\n",
    "            Amenities_pca_df.reset_index(drop=True),\n",
    "            X[self.Image_specials].reset_index(drop=True),\n",
    "            Image_pca_df.reset_index(drop=True),\n",
    "            X[self.Location_specials].reset_index(drop=True),\n",
    "            Location_pca_df.reset_index(drop=True), \n",
    "            NLP_pca_df1.reset_index(drop=True),\n",
    "            NLP_pca_df2.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self,X,y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "            \n",
    "##### modeling \n",
    "class PCA_My_Airbnb_Capstone_Model():\n",
    "    def __init__(self):\n",
    "        # self.amenities_f_list = amenities_f_list\n",
    "        # self.location_f_list = location_f_list\n",
    "        # self.image_f_list = image_f_list\n",
    "        # self.NLP_f_list = NLP_f_list\n",
    "        from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "        from lightgbm import LGBMRegressor\n",
    "        from xgboost import XGBRegressor\n",
    "        from sklearn.model_selection import GridSearchCV, KFold, learning_curve\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        \n",
    "        ### load pipelines\n",
    "        self.amenities_processor = pickle.load(open('./saved_pipelines/amenities_processor.pkl','rb'))\n",
    "        self.image_processor = pickle.load(open('./saved_pipelines/image_processor.pkl','rb'))\n",
    "        self.location_processor = pickle.load(open('./saved_pipelines/location_processor.pkl','rb'))\n",
    "        self.nlp_processor = pickle.load(open('./saved_pipelines/nlp_processor.pkl','rb'))\n",
    "\n",
    "        self.scores = {}\n",
    "        self.pre_processor = PCA_pre_processing_pipeline()\n",
    "        self.x_names = []\n",
    "        self.pipeline_dict = {}\n",
    "        self.params = {}\n",
    "        self.grid_search_dict = {}\n",
    "        self.learning_curve_dict = {}\n",
    "\n",
    "\n",
    "    \n",
    "    def my_train_test_split(self,data):\n",
    "        all_features_no_first3000 = data.iloc[3000:,:] ### discard top 3000: they are used to train CNN. Prevent leakage.\n",
    "        if 'id' in all_features_no_first3000.columns:\n",
    "            del all_features_no_first3000['id'] ## remove id column\n",
    "\n",
    "        #### outlier removal\n",
    "        temp = all_features_no_first3000.iloc[:,[0]]\n",
    "        temp = temp[\n",
    "            (np.log(temp.iloc[:,0]+1) < sorted(np.log(temp.iloc[:,0]+1))[int(temp.shape[0]*0.99)]) &\\\n",
    "            (np.log(temp.iloc[:,0]+1) > sorted(np.log(temp.iloc[:,0]+1))[int(temp.shape[0]*0.01)])\n",
    "        ]\n",
    "        all_features_no_first3000 = all_features_no_first3000[all_features_no_first3000.index.isin(list(temp.index))]\n",
    "        \n",
    "        #### train test split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X = all_features_no_first3000.iloc[:,1:] ## define X\n",
    "        print('Filling infinite values with -1 ...')\n",
    "        X[X.columns] = np.where(np.isinf(X[X.columns]), -1, X[X.columns]) #### filling infinite values with -1\n",
    "\n",
    "        y = all_features_no_first3000.iloc[:,0] ## define y\n",
    "\n",
    "        ## train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "\n",
    "        ### identify the four feature parts\n",
    "        amenities_f_list = [i for i in X_train.columns if i.startswith('Amenities')] ### starts with 'Amenities'\n",
    "        location_f_list = [i for i in X_train.columns if i.startswith('Location')]\n",
    "        image_f_list = [i for i in X_train.columns if i.startswith('Image')]\n",
    "        NLP_f_list = [i for i in X_train.columns if i.startswith('NLP')]\n",
    "\n",
    "        print(dedent(f'''\n",
    "        Amenities raw features count: {len(amenities_f_list)}\n",
    "        Location raw features count: {len(location_f_list)}\n",
    "        Image raw features count: {len(image_f_list)}\n",
    "        NLP raw features count: {len(NLP_f_list)}\n",
    "        '''))\n",
    "\n",
    "        print(dedent(f'''\n",
    "        X_train shape: {X_train.shape}\n",
    "        X_test shape: {X_test.shape}\n",
    "        y_train shape: {y_train.shape}\n",
    "        y_test shape: {y_test.shape}\n",
    "        '''))\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    def get_X_y(self,data):\n",
    "        all_features_no_first3000 = data.iloc[3000:,:] ### discard top 3000: they are used to train CNN. Prevent leakage.\n",
    "        if 'id' in all_features_no_first3000.columns:\n",
    "            del all_features_no_first3000['id'] ## remove id column\n",
    "\n",
    "        #### outlier removal\n",
    "        temp = all_features_no_first3000.iloc[:,[0]]\n",
    "        temp = temp[\n",
    "            (np.log(temp.iloc[:,0]+1) < sorted(np.log(temp.iloc[:,0]+1))[int(temp.shape[0]*0.99)]) &\\\n",
    "            (np.log(temp.iloc[:,0]+1) > sorted(np.log(temp.iloc[:,0]+1))[int(temp.shape[0]*0.01)])\n",
    "        ]\n",
    "        all_features_no_first3000 = all_features_no_first3000[all_features_no_first3000.index.isin(list(temp.index))]\n",
    "        \n",
    "        #### train test split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X = all_features_no_first3000.iloc[:,1:] ## define X\n",
    "        print('Filling infinite values with -1 ...')\n",
    "        X[X.columns] = np.where(np.isinf(X[X.columns]), -1, X[X.columns]) #### filling infinite values with -1\n",
    "\n",
    "        y = all_features_no_first3000.iloc[:,0] ## define y\n",
    "\n",
    "        ### identify the four feature parts\n",
    "        amenities_f_list = [i for i in X.columns if i.startswith('Amenities')] ### starts with 'Amenities'\n",
    "        location_f_list = [i for i in X.columns if i.startswith('Location')]\n",
    "        image_f_list = [i for i in X.columns if i.startswith('Image')]\n",
    "        NLP_f_list = [i for i in X.columns if i.startswith('NLP')]\n",
    "\n",
    "        print(dedent(f'''\n",
    "        Amenities raw features count: {len(amenities_f_list)}\n",
    "        Location raw features count: {len(location_f_list)}\n",
    "        Image raw features count: {len(image_f_list)}\n",
    "        NLP raw features count: {len(NLP_f_list)}\n",
    "        '''))\n",
    "\n",
    "        print(dedent(f'''\n",
    "        X shape: {X.shape}\n",
    "        y shape: {y.shape}\n",
    "        '''))\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def scorer_dict(self,estimator,X,y):\n",
    "#         ## Define scoring functions\n",
    "#         from sklearn.metrics import r2_score, mean_squared_error\n",
    "#         from scipy.stats import spearmanr\n",
    "#         from sklearn.metrics import make_scorer\n",
    "        \n",
    "#         def spearmanr_score(truth, pred):\n",
    "#             return spearmanr(truth, pred)[0]\n",
    "#         pred = estimator.predict(X)\n",
    "#         scorer_dict = {\n",
    "#             'r2':make_scorer(r2_score, greater_is_better=True)(y, pred),\n",
    "#             'mean_squared_error':make_scorer(mean_squared_error, greater_is_better=False)(y, pred),\n",
    "#             'spearmanr':make_scorer(spearmanr_score, greater_is_better=True)(y, pred)\n",
    "#         }\n",
    "        \n",
    "#         return scorer_dict\n",
    "    \n",
    "    def get_scoring_dict(self):\n",
    "        ## Define scoring functions\n",
    "        from sklearn.metrics import r2_score, mean_squared_error\n",
    "        from scipy.stats import spearmanr\n",
    "        from sklearn.metrics import make_scorer\n",
    "        def spearmanr_score(truth, pred):\n",
    "            return spearmanr(truth, pred)[0]\n",
    "        scorer_dict = {\n",
    "            'r2':make_scorer(r2_score, greater_is_better=True),\n",
    "            'mean_squared_error':make_scorer(mean_squared_error, greater_is_better=False),\n",
    "            'spearmanr':make_scorer(spearmanr_score, greater_is_better=True)\n",
    "        }\n",
    "        return scorer_dict\n",
    "\n",
    "    def train(self, X, y):\n",
    "\n",
    "        for n in X.columns:\n",
    "            if '>' in n or '<' in n or ',' in n:\n",
    "                del X[n]\n",
    "            \n",
    "        self.x_names = list(X.columns)\n",
    "#         self.transformed_X_name = list(new_X_train.columns)\n",
    "        \n",
    "        #####\n",
    "        # define quantiles\n",
    "        self.quantiles = [0.05, 0.25, 0.5, 0.75, 0.95]\n",
    "\n",
    "        # fit models for each quantile\n",
    "        self.models = []\n",
    "        for q in tqdm(self.quantiles, desc='Training quantile models'):\n",
    "            \n",
    "            #### grid searching\n",
    "            param_grid = {\n",
    "                'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "                'model__num_leaves': [6, 12, 24, 36],\n",
    "                'model__max_depth': [3, 5, 7, 10],\n",
    "            }\n",
    "            \n",
    "            #### k-fold\n",
    "            cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            \n",
    "            #### define the pipeline\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', self.pre_processor),\n",
    "                ('model', LGBMRegressor(random_state=42,objective='quantile', alpha=q))\n",
    "            ])\n",
    "            \n",
    "            #### apply grid search\n",
    "            print(f'Running GridSearchCV for Quantile {q}')\n",
    "            grid_search = GridSearchCV(pipeline, \n",
    "                                       param_grid, cv=cv, scoring=self.get_scoring_dict(), \n",
    "                                       verbose=2, refit=False, error_score='raise')\n",
    "            grid_search.fit(X, y)\n",
    "            print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "            print(\"Best score:\", grid_search.best_score_)\n",
    "            self.grid_search_dict[q] = grid_search\n",
    "            self.scores[q] = grid_search.best_score_\n",
    "            self.params[q] = grid_search.best_params_\n",
    "            \n",
    "            #### learning curve\n",
    "            print(f'Running learning curve for Quantile {q}')\n",
    "            self.learning_curve_dict[q] = learning_curve(\n",
    "                pipeline, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                scoring=self.scorer_dict, shuffle=True, random_state=42\n",
    "            )\n",
    "\n",
    "            #### final fit\n",
    "            print(f'Final fitting for Quantile {q}')\n",
    "            self.pipeline_dict[q] = Pipeline(steps=[\n",
    "                ('preprocessor', self.pre_processor),\n",
    "                ('model', LGBMRegressor(random_state=42,objective='quantile', alpha=q, **self.params[q]))\n",
    "            ])\n",
    "            self.pipeline_dict[q].fit(X, y)\n",
    "\n",
    "            \n",
    "        ##### model_for_shap\n",
    "        self.model_for_shap = self.pipeline_dict[0.5]\n",
    "        self.model_for_shap_score = self.scores[0.5]\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = X[self.x_names]\n",
    "        preds = {}\n",
    "        for q,pipeline in self.pipeline_dict.items():\n",
    "            pred = pipeline.predict(X)  ## predict\n",
    "            preds[quantile] = pred\n",
    "\n",
    "        return preds\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b97a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read features\n",
    "data = pd.read_csv('./final_features/LA_extracted_all_features_imputed.csv')\n",
    "data = data[\n",
    "    (np.log(data['price']+1)>=sorted(np.log(data['price']+1))[int(0.025*len(data))]) &\\\n",
    "    (np.log(data['price']+1)<=sorted(np.log(data['price']+1))[int(0.975*len(data))])\n",
    "    ] ### remove outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c949b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA_My_Airbnb_Capstone_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15cae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling infinite values with -1 ...\n",
      "\n",
      "Amenities raw features count: 174\n",
      "Location raw features count: 28\n",
      "Image raw features count: 617\n",
      "NLP raw features count: 979\n",
      "\n",
      "\n",
      "X shape: (34827, 1798)\n",
      "y shape: (34827,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = model.get_X_y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfee734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training quantile models:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Quantile 0.05\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n",
      "X[self.Amenities_specials].shape: (11609, 9) Amenities_pca_df.shape: (11609, 10) X[self.Image_specials].shape: (11609, 6) Image_pca_df.shape: (11609, 10) X[self.Location_specials].shape: (11609, 10) Location_pca_df.shape: (11609, 5) NLP_pca_df1.shape: (11609, 20) NLP_pca_df2.shape: (11609, 10)\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__num_leaves=6; total time=  54.1s\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n",
      "X[self.Amenities_specials].shape: (11609, 9) Amenities_pca_df.shape: (11609, 10) X[self.Image_specials].shape: (11609, 6) Image_pca_df.shape: (11609, 10) X[self.Location_specials].shape: (11609, 10) Location_pca_df.shape: (11609, 5) NLP_pca_df1.shape: (11609, 20) NLP_pca_df2.shape: (11609, 10)\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__num_leaves=6; total time=  53.9s\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n",
      "X[self.Amenities_specials].shape: (11609, 9) Amenities_pca_df.shape: (11609, 10) X[self.Image_specials].shape: (11609, 6) Image_pca_df.shape: (11609, 10) X[self.Location_specials].shape: (11609, 10) Location_pca_df.shape: (11609, 5) NLP_pca_df1.shape: (11609, 20) NLP_pca_df2.shape: (11609, 10)\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__num_leaves=6; total time=  54.4s\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n",
      "X[self.Amenities_specials].shape: (11609, 9) Amenities_pca_df.shape: (11609, 10) X[self.Image_specials].shape: (11609, 6) Image_pca_df.shape: (11609, 10) X[self.Location_specials].shape: (11609, 10) Location_pca_df.shape: (11609, 5) NLP_pca_df1.shape: (11609, 20) NLP_pca_df2.shape: (11609, 10)\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__num_leaves=12; total time=  54.4s\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n",
      "X[self.Amenities_specials].shape: (11609, 9) Amenities_pca_df.shape: (11609, 10) X[self.Image_specials].shape: (11609, 6) Image_pca_df.shape: (11609, 10) X[self.Location_specials].shape: (11609, 10) Location_pca_df.shape: (11609, 5) NLP_pca_df1.shape: (11609, 20) NLP_pca_df2.shape: (11609, 10)\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__num_leaves=12; total time=  54.1s\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n",
      "X[self.Amenities_specials].shape: (11609, 9) Amenities_pca_df.shape: (11609, 10) X[self.Image_specials].shape: (11609, 6) Image_pca_df.shape: (11609, 10) X[self.Location_specials].shape: (11609, 10) Location_pca_df.shape: (11609, 5) NLP_pca_df1.shape: (11609, 20) NLP_pca_df2.shape: (11609, 10)\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__num_leaves=12; total time=  53.9s\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n",
      "X[self.Amenities_specials].shape: (11609, 9) Amenities_pca_df.shape: (11609, 10) X[self.Image_specials].shape: (11609, 6) Image_pca_df.shape: (11609, 10) X[self.Location_specials].shape: (11609, 10) Location_pca_df.shape: (11609, 5) NLP_pca_df1.shape: (11609, 20) NLP_pca_df2.shape: (11609, 10)\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__num_leaves=24; total time=  53.9s\n",
      "Amenities_PCA...\n",
      "Image_PCA...\n",
      "Location_PCA...\n",
      "NLP_Embedding_PCA...\n",
      "NLP_NER_PCA...\n",
      "X[self.Amenities_specials].shape: (23218, 9) Amenities_pca_df.shape: (23218, 10) X[self.Image_specials].shape: (23218, 6) Image_pca_df.shape: (23218, 10) X[self.Location_specials].shape: (23218, 10) Location_pca_df.shape: (23218, 5) NLP_pca_df1.shape: (23218, 20) NLP_pca_df2.shape: (23218, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training quantile models:   0%|          | 0/5 [07:03<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2e243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6670e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "    ### train test split\n",
    "    X_train, X_test,y_train,y_test = my_model.my_train_test_split(data) ### take the LA_extracted_all_features_imputed.csv\n",
    "\n",
    "    ### training\n",
    "    my_model.train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b240c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cnn_utiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPCA_My_Airbnb_Capstone_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mPCA_My_Airbnb_Capstone_Model.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m### load pipelines\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamenities_processor \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_pipelines/amenities_processor.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./saved_pipelines/image_processor.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation_processor \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_pipelines/location_processor.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_processor \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_pipelines/nlp_processor.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cnn_utiles'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019a5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28b6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a701e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecaed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59244604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a6325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843affd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e07700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b94c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
